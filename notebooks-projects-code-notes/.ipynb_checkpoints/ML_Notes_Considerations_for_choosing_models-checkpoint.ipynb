{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y - {categorical/continuous} {classification/regression} {labeling, probabilities/value prediction}\n",
    "\n",
    "Prediction/Interpretation \n",
    "* {Gradient Boosting: very good predictor, but many parameters to setup, \n",
    "* Random Forest quick and easy, but identifies the complex relationships. \n",
    "    * Comes with feature importance and effect curves for interpretation. \n",
    "* SVM another good model, but computationally prohibitive, \n",
    "* /LinearModel  - similar to SVM less computationally prohibitive\n",
    "\n",
    "Going for interpretation or raw predictive power. We can make effect curves on any model, but there is overhead work to prepare, and we don’t know there is any confounding, and multi-colinearity. If we can move past multi-colinearity, then we can gather associations but not causality. \n",
    "\n",
    "approach learning in an ensamble method works best\n",
    "\n",
    "Parametric - Forecasting, Extrapolation, Interpretation, \n",
    "* when data far from other data then it feels like there is structure.\n",
    "\n",
    "/NonParametric - will only work with the data space it has seen. It does not do well when we jump from the data space we have data for over to another area where science says the model/rules will apply in that area where we have not seen the data before. \n",
    "* a very flexible model. \n",
    "* no hyperplane, but there is a partition in space. \n",
    "\n",
    "Decision tree is not a good predictive model, but it is used as the foundation of the random forest and gradient boosting. \n",
    "\n",
    "Features:\n",
    "* Categorical\n",
    "    * Problems with SVM, KNN, \n",
    "        * anything that cares about distance. ie. how do we classify importance of heroine vs. chewing gum. both can be yes, but how do we determine which is important. \n",
    "            * this can be setup with a cost/benefit matrix. \n",
    "            * another method is determining the scale (standardizing) such as weighting heroine with a multiplier of 1,000,000 vs. gum. \n",
    "                * but again this is arbitrary and prone to human error and issues. \n",
    "                * can consider this as feature engineering. \n",
    "* Continuous\n",
    "    * probably need to standardize here as well. \n",
    "\n",
    "Regularization: all models have such dials for purposes of limiting overfitting. Even when we don’t use regularization we are choosing to regularize with a parameter of say 0. \n",
    "\n",
    "In statistics the importance is put on quantifying the uncertainty of our estimates. \n",
    "In ML, the importance is placed on most reliable out of sample performance. \n",
    "fitting models to data with varying degrees of accuracy  predicted on the bases of out of sample validations, kfolds etc. through tuning parameters. \n",
    "\n",
    "when fitting a model with best least squares fit, we are choosing the lowest MSE. \n",
    "\n",
    "\n",
    "\n",
    "Model prediction and usage Ordering:\n",
    "* Gradient boosting - very flexible, little bias, but unwieldy with so many parameters. cannot be parallelized because we need to keep track of previous ansers as we go. \n",
    "    * don’t have to standardize\n",
    "    * dummy variable are easy. \n",
    "    * don’t have to create interactions.\n",
    "* Random Forest (RF) \n",
    "    * near cutting edge performance with little attention or input. \n",
    "    * parallelizeable. \n",
    "    * effect curve is maybe the only downside interpretation. \n",
    "    * don’t have to standardize\n",
    "    * dummy variable are easy. \n",
    "    * don’t have to create interactions. \n",
    "* Support Vector Machine (SVM )\n",
    "    * can project into mult-dimensional space and use one cut to get\n",
    "    * parametric model \n",
    "        * some underlying structure between features and outcomes.\n",
    "    * Mostly used for classification. \n",
    "    * Q matrix\n",
    "    * Possibly multi-threaded\n",
    "    * model can be affected by buterfly effect where support vector data points can influence other parts of the model and data way off,\n",
    "        * but a way around this is to modify the c parameter that will allow no data points or as many data points that i want within the margin\n",
    "* KNN\n",
    "    * Computationally taxing because it calculates distance for each data point against all others to find each neighbor. \n",
    "    * elegant idea. \n",
    "    * not good at predicting. \n",
    "* LinearModel\n",
    "    * have to work very hard to build the model\n",
    "    * have to standardize data.\n",
    "    * if we want to interpret coefficients or effect curves we have to eliminate mulit-colinearity. or correlation etc. \n",
    "    * Can do hypothesis testing of importance of the feature. \n",
    "* Decision Tree (DT)\n",
    "* \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
